
# Variational Autoencoder (VAE) for MNIST Digit Reconstruction and Generation

This project implements a **Variational Autoencoder (VAE)** to reconstruct and generate handwritten digits from the MNIST dataset. The VAE is a probabilistic generative model designed to learn a latent representation of the data and reconstruct it effectively.

---

### **Features**
1. **MNIST Reconstruction**: Trains the VAE on the MNIST dataset to reconstruct handwritten digits.
2. **Latent Space Sampling**: Uses the latent space to generate new digits.
3. **Custom Architecture**:
   - Fully connected layers for both the encoder and decoder.
   - ReLU activations in the hidden layers and Sigmoid activation for output reconstruction.
4. **Visualization**: Outputs side-by-side comparisons of original and reconstructed images.
5. **Training Configurations**:
   - Batch size, learning rate, hidden layer size, latent dimension, and epochs are configurable.

---

### **Project Files**
1. **`model.py`**: Contains the VAE model implementation with:
   - An encoder for mapping input data to the latent space.
   - A decoder for reconstructing the input from the latent representation.
   - The reparameterization trick for training the VAE.
2. **`train.py`**: Handles data loading, training, testing, and saving the model. It also generates reconstruction images for comparison.
3. **`image.png`**: A sample image showing reconstructed digits generated by the trained VAE. The left side displays the original digits, while the right side displays the reconstructions.

---

### **Setup and Installation**
1. Clone the repository:
   ```bash
   git clone <repository_url>
   cd <repository_name>
   ```
2. Install dependencies:
   ```bash
   pip install torch torchvision tqdm
   ```
3. Download the MNIST dataset automatically during training.

---

### **Training the Model**
Run the training script:
```bash
python train.py
```
- The script will train the VAE on the MNIST dataset.
- Reconstruction and KL divergence losses are calculated to optimize the model.

During training, reconstructed images are saved in the `image/` directory.

---

### **Model Overview**
- **Input Size**: 28 Ã— 28 pixels (flattened to 784 dimensions).
- **Encoder**:
  - Maps the input to a latent representation (mean and variance).
- **Decoder**:
  - Reconstructs the original input from the sampled latent vector.
- **Latent Space Dimension**: 64.

---

### **Testing and Image Generation**
1. Load the trained model:
   ```bash
   python model.py
   ```
2. Generate reconstructed images:
   - Save them as `image.png` for visualization.

---

### **Results**
The model achieves high-quality reconstructions, as illustrated in `image.png`. The left column shows original digits, while the right column shows the reconstructed digits from the VAE.

---

### **Custom Configurations**
To modify training settings:
- Update `config` in `train.py`:
  ```python
  config = {
      "batch_size": 128,
      "num_epochs": 25,
      "lr": 1e-3,
      "input_size": 784,
      "hidden_size": 128,
      "latent_size": 64,
      "alpha": 0.001
  }
  ```

---

### **Dependencies**
- Python 3.8+
- PyTorch
- torchvision
- tqdm

---

### **Acknowledgments**
This implementation is inspired by the original Variational Autoencoder paper by Kingma and Welling (2013).

---

For any questions or contributions, feel free to open an issue or submit a pull request!
